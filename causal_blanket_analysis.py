#!/usr/bin/env python
"""
causal_blanket_analysis.py

Script to load time-series data collected during particle lenia illumination, 
analyze the data using information theory metrics, and calculate causal blanket statistics.

Usage:
  python causal_blanket_analysis.py \
      --data_dir="./data/particle_lenia_illumination" \
      --out_csv="causal_blanket_analysis.csv" \
      --n_analyze=10
"""

import os
import argparse
import pickle
import numpy as np
from tqdm import tqdm
import pandas as pd
import jax.numpy as jnp
from matplotlib import pyplot as plt

# Import information theory utilities for mutual information and PID calculation
import info_theory_utils

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--data_dir", type=str, default="./data/particle_lenia_illumination",
                        help="Directory containing causal_blanket_data.pkl generated by illuminate_particle_lenia.py")
    parser.add_argument("--out_csv", type=str, default="causal_blanket_analysis.csv",
                        help="Output CSV filename")
    parser.add_argument("--n_analyze", type=int, default=None,
                        help="Number of individuals to analyze (None = analyze all available)")
    parser.add_argument("--bins", type=int, default=10,
                        help="Number of bins for histogram-based information theory calculations")
    parser.add_argument("--time_lag", type=int, default=1, 
                        help="Time lag for causal analysis (e.g., t to t+1)")
    parser.add_argument("--plot", action="store_true",
                        help="Generate plots for analysis")
    parser.add_argument("--plot_dir", type=str, default="./causal_analysis_plots",
                        help="Directory to save plots")
    parser.add_argument("--min_timesteps", type=int, default=10,
                        help="Minimum number of timesteps required for analysis")
    parser.add_argument("--partition_method", type=str, default="spatial_halves",
                        choices=["spatial_halves", "density", "velocity", "clusters"],
                        help="Method to partition particles into X and Y regions")
    return parser.parse_args()

########################################
# 1. Particle position data extraction functions
########################################
def load_state_histories(data_dir):
    """
    Load saved state histories from causal_blanket_data.pkl.
    
    Returns:
        dict containing state histories, parameters, and indices
    """
    # Try to load causal_blanket_data.pkl first (preferred)
    cb_data_path = os.path.join(data_dir, "causal_blanket_data.pkl")
    if (os.path.exists(cb_data_path)):
        print(f"Loading causal blanket data from {cb_data_path}")
        with open(cb_data_path, "rb") as f:
            cb_data = pickle.load(f)
        return cb_data
    
    # Fallback to pop.pkl if causal_blanket_data.pkl doesn't exist
    pop_path = os.path.join(data_dir, "pop.pkl")
    if not os.path.exists(pop_path):
        raise FileNotFoundError(f"Could not find causal_blanket_data.pkl or pop.pkl in {data_dir}")
    
    print(f"Loading population data from {pop_path}")
    with open(pop_path, "rb") as f:
        pop_data = pickle.load(f)
        
    # Check if states are available in pop.pkl
    if 'states' not in pop_data:
        raise ValueError(f"No state history found in {pop_path}. "
                         "Run illuminate_particle_lenia.py with --save_time_series flag first.")
    
    return {
        'states': pop_data['states'],
        'params': pop_data['params'],
        'indices': list(range(len(pop_data['states'])))
    }

def extract_particle_data(state_history):
    """
    Extract particle positions from state history.
    
    Args:
        state_history: State history as saved by illuminate_particle_lenia.py
                     Expected format is (timesteps, num_particles, features)
    
    Returns:
        Particle position array (timesteps, num_particles, 2)
    """
    # Handle different possible formats
    if isinstance(state_history, dict):
        # If state history is a dict with 'pos' key (typical for particle systems)
        if 'pos' in state_history:
            return np.array(state_history['pos'])
        # If it has 'x' and 'y' keys
        elif 'x' in state_history and 'y' in state_history:
            x = np.array(state_history['x'])
            y = np.array(state_history['y'])
            return np.stack([x, y], axis=-1)
    
    # If already in correct format (timesteps, particles, features)
    if isinstance(state_history, np.ndarray) or isinstance(state_history, jnp.ndarray):
        if state_history.ndim == 3:
            return np.array(state_history)
    
    # If it's a JAX DeviceArray, convert to numpy
    if hasattr(state_history, 'shape') and state_history.ndim == 3:
        return np.array(state_history)
    
    raise ValueError(f"Unexpected state history format: {type(state_history)}")

########################################
# 2. Partition particles into X and Y regions
########################################
def partition_XY(particle_positions, method='spatial_halves'):
    """
    Partition particles into X and Y regions to analyze causal relationships.
    
    Args:
        particle_positions: Array of shape (timesteps, num_particles, 2) with x,y positions
        method: Method to use for partitioning:
                'spatial_halves' - Split based on x-coordinate (left/right)
                'density' - Based on local density
                'velocity' - Based on particle velocities
                'clusters' - Based on k-means clustering
    
    Returns:
        X_series, Y_series: Time series of features for both partitions
    """
    timesteps, num_particles, dims = particle_positions.shape
    info_theory_utils.visualize_data_shape("particle_positions", particle_positions)
    
    if method == 'spatial_halves':
        # For each timestep, count particles in left vs right half of space
        X_series = np.zeros(timesteps)
        Y_series = np.zeros(timesteps)
        
        # Get domain bounds
        x_min = particle_positions[:, :, 0].min()
        x_max = particle_positions[:, :, 0].max()
        midpoint = (x_min + x_max) / 2
        
        for t in range(timesteps):
            x_pos = particle_positions[t, :, 0]
            left_mask = x_pos < midpoint
            X_series[t] = np.sum(left_mask)
            Y_series[t] = num_particles - X_series[t]
        
    elif method == 'density':
        # Calculate density-based features
        X_series = np.zeros(timesteps)  # Will store average nearest neighbor distance in left half
        Y_series = np.zeros(timesteps)  # Will store average nearest neighbor distance in right half
        
        # Get domain bounds
        x_min = particle_positions[:, :, 0].min()
        x_max = particle_positions[:, :, 0].max()
        midpoint = (x_min + x_max) / 2
        
        for t in range(timesteps):
            positions = particle_positions[t]
            x_pos = positions[:, 0]
            
            # Split particles into left and right groups
            left_mask = x_pos < midpoint
            right_mask = ~left_mask
            
            left_positions = positions[left_mask]
            right_positions = positions[right_mask]
            
            # Calculate average nearest neighbor distance for each group
            # (proxy for local density)
            if len(left_positions) > 1:
                # Calculate pairwise distances
                diffs = left_positions[:, np.newaxis, :] - left_positions[np.newaxis, :, :]
                sq_dists = np.sum(diffs ** 2, axis=2)
                np.fill_diagonal(sq_dists, np.inf)  # Ignore self-distances
                X_series[t] = np.min(sq_dists, axis=1).mean()  # Average nearest neighbor distance
            else:
                X_series[t] = 0
                
            if len(right_positions) > 1:
                diffs = right_positions[:, np.newaxis, :] - right_positions[np.newaxis, :, :]
                sq_dists = np.sum(diffs ** 2, axis=2)
                np.fill_diagonal(sq_dists, np.inf)
                Y_series[t] = np.min(sq_dists, axis=1).mean()
            else:
                Y_series[t] = 0
    
    elif method == 'velocity':
        # Use particle velocities to partition
        X_series = np.zeros(timesteps-1)
        Y_series = np.zeros(timesteps-1)
        
        # Calculate velocities
        velocities = particle_positions[1:] - particle_positions[:-1]
        speed = np.sqrt(np.sum(velocities**2, axis=2))  # (timesteps-1, num_particles)
        
        # Get mean velocity magnitude for each timestep
        for t in range(timesteps-1):
            # Sort particles by speed
            sorted_idx = np.argsort(speed[t])
            half_idx = num_particles // 2
            
            # Slower half goes to X, faster half goes to Y
            X_series[t] = np.mean(speed[t, sorted_idx[:half_idx]])
            Y_series[t] = np.mean(speed[t, sorted_idx[half_idx:]])
    
    elif method == 'clusters':
        # Use k-means clustering to partition particles
        try:
            from sklearn.cluster import KMeans
            X_series = np.zeros(timesteps)
            Y_series = np.zeros(timesteps)
            
            for t in range(timesteps):
                positions = particle_positions[t]
                
                # Apply k-means clustering (2 clusters)
                kmeans = KMeans(n_clusters=2, random_state=0).fit(positions)
                labels = kmeans.labels_
                
                # Get mean position of each cluster
                X_series[t] = np.mean(positions[labels == 0, 0])  # x-coordinate of first cluster center
                Y_series[t] = np.mean(positions[labels == 1, 0])  # x-coordinate of second cluster center
                
        except ImportError:
            print("sklearn not available, falling back to spatial_halves method")
            return partition_XY(particle_positions, method='spatial_halves')
    
    else:
        raise ValueError(f"Unknown partitioning method: {method}")
    
    # Add small random perturbation if series has no variance
    if np.var(X_series) < 1e-10:
        print("Warning: X_series has no variance. Adding small random perturbation.")
        X_series += np.random.normal(0, 0.1, len(X_series))
        
    if np.var(Y_series) < 1e-10:
        print("Warning: Y_series has no variance. Adding small random perturbation.")
        Y_series += np.random.normal(0, 0.1, len(Y_series))
    
    info_theory_utils.visualize_data_shape("X_series", X_series)
    info_theory_utils.visualize_data_shape("Y_series", Y_series)
    return X_series, Y_series

########################################
# 3. Causal blanket estimation
########################################
def estimate_causal_blanket(X_series, Y_series, time_lag=1, bins=10):
    """
    Estimate causal blanket components based on time-lagged mutual information.
    
    This implements a simplified version of the reD-BaSS approach described by:
    Rosas et al. (2020): "Reconciling emergences: An information-theoretic approach 
    to identify causal emergence in multivariate data"
    
    Args:
        X_series, Y_series: Time series data for both partitions
        time_lag: Time lag to use for causal analysis
        bins: Number of bins for histogram-based MI calculation
    
    Returns:
        Dict containing causal blanket components and metrics
    """
    # Prepare time-lagged data
    T = len(X_series) - time_lag
    if T <= 0:
        raise ValueError(f"Time series too short ({len(X_series)}) for time lag {time_lag}")
        
    # Current state (time t)
    Xt = X_series[:T]
    Yt = Y_series[:T]
    
    # Future state (time t+time_lag)
    Xt_future = X_series[time_lag:]
    Yt_future = Y_series[time_lag:]
    
    # Debug shapes
    info_theory_utils.visualize_data_shape("Xt", Xt)
    info_theory_utils.visualize_data_shape("Yt", Yt)
    info_theory_utils.visualize_data_shape("Xt_future", Xt_future)
    info_theory_utils.visualize_data_shape("Yt_future", Yt_future)
    
    # Calculate mutual information between current and future states
    mi_X_to_X = info_theory_utils.mutual_information(Xt, Xt_future, bins=bins)
    mi_Y_to_X = info_theory_utils.mutual_information(Yt, Xt_future, bins=bins)
    mi_X_to_Y = info_theory_utils.mutual_information(Xt, Yt_future, bins=bins)
    mi_Y_to_Y = info_theory_utils.mutual_information(Yt, Yt_future, bins=bins)
    
    # Calculate conditional mutual information
    cmi_X_to_X_given_Y = info_theory_utils.conditional_mutual_information(Xt, Xt_future, Yt, bins=bins)
    cmi_Y_to_Y_given_X = info_theory_utils.conditional_mutual_information(Yt, Yt_future, Xt, bins=bins)
    
    # Calculate partial information decomposition
    pid_X = info_theory_utils.pid_2x1y(Xt, Yt, Xt_future, bins=bins)
    pid_Y = info_theory_utils.pid_2x1y(Xt, Yt, Yt_future, bins=bins)
    
    # Return all calculated metrics
    return {
        "MI_X_to_X": mi_X_to_X,
        "MI_Y_to_X": mi_Y_to_X,
        "MI_X_to_Y": mi_X_to_Y, 
        "MI_Y_to_Y": mi_Y_to_Y,
        "CMI_X_to_X_given_Y": cmi_X_to_X_given_Y,
        "CMI_Y_to_Y_given_X": cmi_Y_to_Y_given_X,
        "PID_X": pid_X,
        "PID_Y": pid_Y,
        "Independence": (cmi_X_to_X_given_Y / mi_X_to_X if mi_X_to_X > 0 else 0) + 
                        (cmi_Y_to_Y_given_X / mi_Y_to_Y if mi_Y_to_Y > 0 else 0),
        "Interdependence": pid_X["synergy"] + pid_Y["synergy"],
        "Integration": pid_X["redundancy"] + pid_Y["redundancy"],
    }

########################################
# 4. Calculate synergy measures D and G
########################################
def calc_synergy_measures(X_series, Y_series, time_lag=1, bins=10):
    """
    Calculate synergy measures D and G based on PID components.
    
    Args:
        X_series, Y_series: Time series data for both partitions
        time_lag: Time lag for causal analysis
        bins: Number of bins for histogram calculations
    
    Returns:
        Dict with D and G measures
    """
    # Prepare time-lagged data
    T = len(X_series) - time_lag
    if T <= 0:
        raise ValueError(f"Time series too short ({len(X_series)}) for time lag {time_lag}")
    
    # Current state (time t)
    Xt = X_series[:T]
    Yt = Y_series[:T]
    
    # Joint future state (time t+time_lag)
    Xt_future = X_series[time_lag:]
    Yt_future = Y_series[time_lag:]
    
    # Calculate PID for X future
    pid_X = info_theory_utils.pid_2x1y(Xt, Yt, Xt_future, bins=bins)
    
    # Calculate PID for Y future
    pid_Y = info_theory_utils.pid_2x1y(Xt, Yt, Yt_future, bins=bins)
    
    # Calculate PID for joint future (X,Y)
    # (Need to stack the future values)
    XYt_future = np.column_stack((Xt_future, Yt_future))
    pid_XY = info_theory_utils.pid_2x1y(Xt, Yt, XYt_future, bins=bins)
    
    # Calculate D and G measures
    # D = Synergy - Redundancy (information gain from integration)
    D_X = pid_X["synergy"] - pid_X["redundancy"] 
    D_Y = pid_Y["synergy"] - pid_Y["redundancy"]
    D_XY = pid_XY["synergy"] - pid_XY["redundancy"]
    
    # G = Synergy / (Synergy + Redundancy) (relative strength of synergistic effects)
    G_X = pid_X["synergy"] / (pid_X["synergy"] + pid_X["redundancy"]) if (pid_X["synergy"] + pid_X["redundancy"]) > 0 else 0
    G_Y = pid_Y["synergy"] / (pid_Y["synergy"] + pid_Y["redundancy"]) if (pid_Y["synergy"] + pid_Y["redundancy"]) > 0 else 0
    G_XY = pid_XY["synergy"] / (pid_XY["synergy"] + pid_XY["redundancy"]) if (pid_XY["synergy"] + pid_XY["redundancy"]) > 0 else 0
    
    return {
        "D_X": D_X,
        "D_Y": D_Y,
        "D_XY": D_XY,
        "G_X": G_X,
        "G_Y": G_Y,
        "G_XY": G_XY,
    }

########################################
# 5. Plot utilities
########################################
def plot_causal_analysis(particle_positions, X_series, Y_series, cb_metrics, synergy_metrics, 
                         index, out_dir):
    """Generate plots for causal analysis results"""
    if not os.path.exists(out_dir):
        os.makedirs(out_dir)
        
    # Plot 1: Particle positions over time with animation
    timesteps = particle_positions.shape[0]
    sample_times = np.linspace(0, timesteps-1, min(5, timesteps), dtype=int)
    
    plt.figure(figsize=(15, 10))
    for i, t in enumerate(sample_times):
        plt.subplot(2, 3, i+1)
        
        # Get domain bounds
        x_min = particle_positions[:, :, 0].min()
        x_max = particle_positions[:, :, 0].max()
        midpoint = (x_min + x_max) / 2
        
        # Draw vertical line to indicate partitioning
        plt.axvline(x=midpoint, color='r', linestyle='--', alpha=0.5)
        
        # Color particles based on their side
        x_pos = particle_positions[t, :, 0]
        left_mask = x_pos < midpoint
        
        # Plot left and right particles with different colors
        plt.scatter(particle_positions[t, left_mask, 0], 
                   particle_positions[t, left_mask, 1], 
                   alpha=0.6, s=5, color='blue', label='X region' if i == 0 else None)
        plt.scatter(particle_positions[t, ~left_mask, 0], 
                   particle_positions[t, ~left_mask, 1], 
                   alpha=0.6, s=5, color='red', label='Y region' if i == 0 else None)
        
        plt.title(f"Time step {t}")
        plt.xlabel("X position")
        plt.ylabel("Y position")
        if i == 0:
            plt.legend()
    
    plt.tight_layout()
    plt.savefig(os.path.join(out_dir, f"particle_positions_{index}.png"))
    plt.close()
    
    # Plot 2: X and Y time series with shaded regions
    plt.figure(figsize=(15, 6))
    timesteps_range = np.arange(len(X_series))
    
    plt.plot(timesteps_range, X_series, 'b-', label="X region", linewidth=2)
    plt.plot(timesteps_range, Y_series, 'r-', label="Y region", linewidth=2)
    
    # Add shaded regions to indicate variance
    if len(X_series) > 1:
        x_std = np.std(X_series)
        y_std = np.std(Y_series)
        plt.fill_between(timesteps_range, X_series - x_std, X_series + x_std, color='blue', alpha=0.2)
        plt.fill_between(timesteps_range, Y_series - y_std, Y_series + y_std, color='red', alpha=0.2)
    
    plt.title("Time series of X and Y regions")
    plt.xlabel("Time step")
    plt.ylabel("Value")
    plt.grid(True, alpha=0.3)
    plt.legend()
    plt.savefig(os.path.join(out_dir, f"time_series_{index}.png"))
    plt.close()
    
    # Plot 3: Key information-theoretic metrics as heatmap
    plt.figure(figsize=(10, 8))
    
    # Create matrix of metrics for heatmap
    metric_names = [
        "MI_X_to_X", "MI_Y_to_Y", "MI_X_to_Y", "MI_Y_to_X",
        "CMI_X_to_X_given_Y", "CMI_Y_to_Y_given_X"
    ]
    
    metric_values = [cb_metrics.get(name, 0) for name in metric_names]
    
    # Reshape to square-like matrix for better visualization
    n = int(np.ceil(np.sqrt(len(metric_names))))
    heatmap_data = np.zeros((n, n))
    for i, val in enumerate(metric_values):
        row, col = i // n, i % n
        heatmap_data[row, col] = val
    
    # Plot heatmap
    im = plt.imshow(heatmap_data, cmap='viridis')
    plt.colorbar(im, label='Information (bits)')
    
    # Add text annotations
    for i in range(len(metric_names)):
        row, col = i // n, i % n
        plt.text(col, row, f"{metric_names[i]}\n{metric_values[i]:.4f}", 
                ha="center", va="center", color="white" if heatmap_data[row, col] > 0.5 else "black")
    
    plt.title("Information Transfer Metrics")
    plt.tight_layout()
    plt.savefig(os.path.join(out_dir, f"info_metrics_{index}.png"))
    plt.close()
    
    # Plot 4: Synergy metrics with visual comparison
    plt.figure(figsize=(12, 8))
    
    # Plot bar chart
    synergy_values = [
        synergy_metrics["D_X"], synergy_metrics["D_Y"], synergy_metrics["D_XY"],
        synergy_metrics["G_X"], synergy_metrics["G_Y"], synergy_metrics["G_XY"]
    ]
    synergy_labels = ["D_X", "D_Y", "D_XY", "G_X", "G_Y", "G_XY"]
    
    # Add PID components
    pid_x_values = [cb_metrics["PID_X"]["unique1"], cb_metrics["PID_X"]["unique2"], 
                   cb_metrics["PID_X"]["redundancy"], cb_metrics["PID_X"]["synergy"]]
    pid_x_labels = ["X unique1", "X unique2", "X redundancy", "X synergy"]
    
    pid_y_values = [cb_metrics["PID_Y"]["unique1"], cb_metrics["PID_Y"]["unique2"], 
                   cb_metrics["PID_Y"]["redundancy"], cb_metrics["PID_Y"]["synergy"]]
    pid_y_labels = ["Y unique1", "Y unique2", "Y redundancy", "Y synergy"]
    
    # Combine all values for better visualization
    all_values = synergy_values + pid_x_values + pid_y_values
    all_labels = synergy_labels + pid_x_labels + pid_y_labels
    
    # Set up colors based on value types
    colors = ['#1f77b4'] * 3 + ['#ff7f0e'] * 3 + ['#2ca02c'] * 4 + ['#d62728'] * 4
    
    # Create bar chart with annotations
    bars = plt.bar(all_labels, all_values, color=colors)
    plt.xticks(rotation=45, ha='right')
    
    # Add value labels on top of bars
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{height:.4f}', ha='center', va='bottom', rotation=0)
    
    # Add section divider lines
    plt.axvline(x=2.5, color='black', linestyle='--', alpha=0.3)
    plt.axvline(x=5.5, color='black', linestyle='--', alpha=0.3)
    plt.axvline(x=9.5, color='black', linestyle='--', alpha=0.3)
    
    plt.title("Information Decomposition Metrics")
    plt.ylabel("Information (bits)")
    plt.tight_layout()
    plt.savefig(os.path.join(out_dir, f"synergy_metrics_{index}.png"))
    plt.close()
    
    # Plot 5: System diagram showing information flow between partitions
    plt.figure(figsize=(10, 8))
    
    # Create a simple diagram
    plt.axes([0.1, 0.1, 0.8, 0.8])
    plt.axis('off')
    
    # Draw X and Y as circles
    circle_x = plt.Circle((0.3, 0.5), 0.2, fill=True, alpha=0.3, color='blue', label='X')
    circle_y = plt.Circle((0.7, 0.5), 0.2, fill=True, alpha=0.3, color='red', label='Y')
    plt.gca().add_patch(circle_x)
    plt.gca().add_patch(circle_y)
    
    # Add labels
    plt.text(0.3, 0.5, 'X', ha='center', va='center', fontsize=20)
    plt.text(0.7, 0.5, 'Y', ha='center', va='center', fontsize=20)
    
    # Draw arrows with MI values
    arrow_properties = dict(arrowstyle="->", color="black", linewidth=2)
    
    # X->X arrow
    plt.annotate(f"MI(X→X): {cb_metrics['MI_X_to_X']:.4f}", 
                xy=(0.2, 0.7), xytext=(0.2, 0.9),
                arrowprops=arrow_properties)
    
    # Y->Y arrow
    plt.annotate(f"MI(Y→Y): {cb_metrics['MI_Y_to_Y']:.4f}", 
                xy=(0.8, 0.7), xytext=(0.8, 0.9),
                arrowprops=arrow_properties)
    
    # X->Y arrow
    plt.annotate(f"MI(X→Y): {cb_metrics['MI_X_to_Y']:.4f}", 
                xy=(0.6, 0.5), xytext=(0.4, 0.5),
                arrowprops=arrow_properties)
    
    # Y->X arrow
    plt.annotate(f"MI(Y→X): {cb_metrics['MI_Y_to_X']:.4f}", 
                xy=(0.4, 0.4), xytext=(0.6, 0.4),
                arrowprops=arrow_properties)
    
    plt.title("Information Flow Diagram")
    plt.savefig(os.path.join(out_dir, f"info_flow_{index}.png"))
    plt.close()

########################################
# Main function
########################################
def main():
    args = parse_args()
    
    # Create plot directory if needed
    if args.plot and not os.path.exists(args.plot_dir):
        os.makedirs(args.plot_dir)

    # Load state histories
    cb_data = load_state_histories(args.data_dir)
    state_histories = cb_data['states']
    params = cb_data['params']
    indices = cb_data['indices']
    
    # Limit number of individuals to analyze if specified
    n_individuals = len(state_histories)
    if args.n_analyze is not None:
        n_analyze = min(n_individuals, args.n_analyze)
    else:
        n_analyze = n_individuals
    
    print(f"Analyzing {n_analyze} individuals out of {n_individuals}")
    
    results = []
    for i in tqdm(range(n_analyze)):
        # Get state history and parameters for this individual
        state_hist = state_histories[i]
        param_vector = params[i] if isinstance(params, list) else params[indices[i]]
        
        # Extract particle position data
        try:
            particle_positions = extract_particle_data(state_hist)
        except ValueError as e:
            print(f"Error extracting particle data from individual {i}: {e}")
            continue
            
        # Check if time series is long enough
        timesteps = particle_positions.shape[0]
        if timesteps < args.min_timesteps:
            print(f"Warning: Individual {i} has only {timesteps} timesteps, which is less than the minimum {args.min_timesteps}.")
            print("Consider collecting longer time series or reducing --min_timesteps parameter.")
            if timesteps < 3:  # Absolute minimum for time-lagged analysis
                print(f"Skipping individual {i} due to insufficient timesteps.")
                continue
        
        # Partition into X and Y regions using specified method
        X_series, Y_series = partition_XY(particle_positions, method=args.partition_method)
        
        # Calculate causal blanket metrics
        cb_metrics = estimate_causal_blanket(X_series, Y_series, 
                                           time_lag=args.time_lag, 
                                           bins=args.bins)
        
        # Calculate synergy measures
        synergy_metrics = calc_synergy_measures(X_series, Y_series,
                                              time_lag=args.time_lag,
                                              bins=args.bins)
        
        # Create result dictionary
        result = {
            "index": indices[i],
            "param_0": param_vector[0] if hasattr(param_vector, "__len__") else param_vector,
            "param_1": param_vector[1] if hasattr(param_vector, "__len__") and len(param_vector) > 1 else None,
            "timesteps": timesteps,  # Add number of timesteps to results
        }
        
        # Add causal blanket metrics
        for k, v in cb_metrics.items():
            if not isinstance(v, dict):
                result[k] = v
        
        # Add PID metrics for X
        for k, v in cb_metrics["PID_X"].items():
            result[f"PID_X_{k}"] = v
            
        # Add PID metrics for Y
        for k, v in cb_metrics["PID_Y"].items():
            result[f"PID_Y_{k}"] = v
            
        # Add synergy metrics
        for k, v in synergy_metrics.items():
            result[k] = v
            
        results.append(result)
        
        # Generate plots if requested
        if args.plot:
            plot_causal_analysis(
                particle_positions, X_series, Y_series, 
                cb_metrics, synergy_metrics, 
                indices[i], args.plot_dir
            )

    # Create DataFrame
    df = pd.DataFrame(results)
    print(df.head())
    
    # Save to CSV
    df.to_csv(args.out_csv, index=False)
    print(f"Saved analysis to {args.out_csv}")
    
    # Calculate and print summary statistics
    print("\nSummary Statistics:")
    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns
    summary = df[numeric_cols].describe()
    print(summary)
    
    # Print top 3 individuals by synergy
    print("\nTop 3 individuals by D_XY (synergy):")
    top_by_synergy = df.sort_values("D_XY", ascending=False).head(3)[["index", "D_XY", "G_XY"]]
    print(top_by_synergy)

if __name__ == "__main__":
    main()
